#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üéØ ICT CONFIDENCE CALIBRATOR - Sprint 1.6
=========================================

OBJETIVO: Mejorar el motor de confianza del 45% actual hacia 70%+

ESTRATEGIAS DE CALIBRACI√ìN:
1. üîß Optimizaci√≥n de pesos (weights)
2. üìä Mejora de algoritmos de scoring
3. üéØ Calibraci√≥n de umbrales
4. ‚ö° Optimizaci√≥n de confluencias POI-ICT
5. üìà Validaci√≥n en tiempo real

Versi√≥n: v1.0.0 - Sprint 1.6
Fecha: 04 Agosto 2025
Autor: ICT Engine Team
"""

from sistema.sic import sys
from sistema.sic import os
from sistema.sic import datetime
from sistema.sic import Dict, List, Optional, Tuple
import copy
# MIGRADO A SLUC v2.1
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from sistema.sic import enviar_senal_log

try:
    from core.ict_engine.confidence_engine import CONFIDENCE_CONFIG
    from core.ict_engine.confidence_engine import confidence_engine as global_confidence_engine
except ImportError as e:
    try:
        # Fallback para imports relativos
        from .confidence_engine import CONFIDENCE_CONFIG
        from .confidence_engine import confidence_engine as global_confidence_engine
    except ImportError as e2:
        enviar_senal_log("ERROR", f"Error importando confidence engine: {e2}", __name__, "confidence_calibrator")
        sys.exit(1)


class ConfidenceCalibrator:
    """
    üéØ Calibrador avanzado para el motor de confianza ICT

    Meta: 45% ‚Üí 70%+ confianza promedio
    """

    def __init__(self):
        """Inicializar calibrador"""
        self.original_config = copy.deepcopy(CONFIDENCE_CONFIG)
        self.current_config = copy.deepcopy(CONFIDENCE_CONFIG)
        self.calibration_history = []
        self.best_config = None
        self.best_score = 0.0

        enviar_senal_log("INFO", "üéØ Confidence Calibrator inicializado - Sprint 1.6", __name__, "confidence_calibrator")
        enviar_senal_log("INFO", f"üìä Configuraci√≥n original cargada: {len(self.original_config)} par√°metros", __name__, "confidence_calibrator")

    def analyze_current_performance(self) -> Dict:
        """
        üìä Analiza el rendimiento actual del motor de confianza
        """
        try:
            enviar_senal_log("INFO", "üìä Analizando rendimiento actual del motor de confianza...", __name__, "confidence_calibrator")

            # Obtener estad√≠sticas del motor global
            stats = global_confidence_engine.get_engine_stats()

            current_performance = {
                'avg_confidence': stats.get('avg_confidence', 0.0),
                'calculations_total': stats.get('calculations_total', 0),
                'patterns_analyzed': stats.get('patterns_analyzed', {}),
                'confidence_distribution': self._analyze_confidence_distribution(stats),
                'performance_grade': self._calculate_performance_grade(stats.get('avg_confidence', 0.0)),
                'improvement_needed': max(0.0, 0.70 - stats.get('avg_confidence', 0.0)),
                'timestamp': datetime.now().isoformat()
            }

            enviar_senal_log("INFO",
                            f"üìà Rendimiento actual: {current_performance['avg_confidence']:.1%} "
                            f"(Grade: {current_performance['performance_grade']}) | "
                            f"Mejora necesaria: +{current_performance['improvement_needed']:.1%}",
                            __name__, "confidence_calibrator")

            return current_performance

        except Exception as e:
            enviar_senal_log("ERROR", f"Error analizando rendimiento: {e}", __name__, "confidence_calibrator")
            return {'error': str(e), 'avg_confidence': 0.0}

    def _analyze_confidence_distribution(self, stats: Dict) -> Dict:
        """Analiza la distribuci√≥n de confianza"""
        # Simulaci√≥n de distribuci√≥n basada en promedio
        avg_conf = stats.get('avg_confidence', 0.0)

        return {
            'very_high': max(0, (avg_conf - 0.85) * 100) if avg_conf > 0.85 else 0,
            'high': max(0, min(15, (avg_conf - 0.75) * 100)) if avg_conf > 0.75 else 0,
            'medium': max(0, min(25, (avg_conf - 0.65) * 100)) if avg_conf > 0.65 else 30,
            'low': max(0, min(35, (avg_conf - 0.50) * 100)) if avg_conf > 0.50 else 40,
            'very_low': max(0, 100 - (avg_conf * 100)) if avg_conf < 0.50 else 30
        }

    def _calculate_performance_grade(self, avg_confidence: float) -> str:
        """Calcula el grade de rendimiento"""
        if avg_confidence >= 0.85:
            return "A+ (EXCELENTE)"
        elif avg_confidence >= 0.75:
            return "A (MUY BUENO)"
        elif avg_confidence >= 0.65:
            return "B (BUENO)"
        elif avg_confidence >= 0.55:
            return "C (REGULAR)"
        elif avg_confidence >= 0.45:
            return "D (BAJO)"
        else:
            return "F (MUY BAJO)"

    def generate_optimized_configs(self) -> List[Dict]:
        """
        üîß Genera configuraciones optimizadas para testing

        Estrategias:
        1. Aumentar peso de confluencia POI (mejor sinergia)
        2. Mejorar multiplicadores de sesi√≥n
        3. Optimizar umbrales de confianza
        4. Ajustar algoritmos de scoring
        """
        optimized_configs = []

        # CONFIG 1: ENFOQUE POI-ICT SINERGIA
        config_1 = copy.deepcopy(self.current_config)
        config_1['weights'] = {
            'base_pattern': 0.35,        # Reducir peso base
            'poi_confluence': 0.35,      # ‚≠ê AUMENTAR confluencia POI (25% ‚Üí 35%)
            'historical': 0.15,          # Mantener hist√≥rico
            'market_structure': 0.10,    # Mantener estructura
            'session_context': 0.05,     # Reducir sesi√≥n
        }
        config_1['confluence_distance_pips'] = 15  # Aumentar rango confluencia
        config_1['description'] = "POI-ICT Sinergia Mejorada"
        optimized_configs.append(config_1)

        # CONFIG 2: ENFOQUE SESI√ìN Y LIQUIDEZ
        config_2 = copy.deepcopy(self.current_config)
        config_2['session_multipliers'] = {
            'asian': 0.90,           # ‚≠ê Mejorar asi√°tica (0.85 ‚Üí 0.90)
            'london': 1.20,          # ‚≠ê Mejorar Londres (1.1 ‚Üí 1.20)
            'new_york': 1.10,        # ‚≠ê Mejorar NY (1.0 ‚Üí 1.10)
            'overlap': 1.25,         # ‚≠ê Mejorar overlap (1.15 ‚Üí 1.25)
            'quiet': 0.75,           # ‚≠ê Mejorar quiet (0.7 ‚Üí 0.75)
        }
        config_2['description'] = "Sesiones Optimizadas"
        optimized_configs.append(config_2)

        # CONFIG 3: ENFOQUE HIST√ìRICO Y ESTRUCTURA
        config_3 = copy.deepcopy(self.current_config)
        config_3['weights'] = {
            'base_pattern': 0.30,        # Reducir base
            'poi_confluence': 0.25,      # Mantener POI
            'historical': 0.25,          # ‚≠ê AUMENTAR hist√≥rico (15% ‚Üí 25%)
            'market_structure': 0.15,    # ‚≠ê AUMENTAR estructura (10% ‚Üí 15%)
            'session_context': 0.05,     # Reducir sesi√≥n
        }
        config_3['description'] = "Hist√≥rico y Estructura Mejorados"
        optimized_configs.append(config_3)

        # CONFIG 4: ENFOQUE AGRESIVO (META 70%+)
        config_4 = copy.deepcopy(self.current_config)
        config_4['weights'] = {
            'base_pattern': 0.25,        # ‚≠ê Reducir significativamente base
            'poi_confluence': 0.40,      # ‚≠ê M√ÅXIMO peso a confluencia POI
            'historical': 0.20,          # ‚≠ê Aumentar hist√≥rico
            'market_structure': 0.10,    # Mantener estructura
            'session_context': 0.05,     # M√≠nimo sesi√≥n
        }
        config_4['confluence_distance_pips'] = 20  # ‚≠ê M√°ximo rango confluencia
        config_4['session_multipliers'] = {
            'asian': 0.95,
            'london': 1.25,
            'new_york': 1.15,
            'overlap': 1.30,
            'quiet': 0.80,
        }
        config_4['description'] = "Configuraci√≥n Agresiva Meta 70%+"
        optimized_configs.append(config_4)

        # CONFIG 5: CONFIGURACI√ìN BALANCEADA MEJORADA
        config_5 = copy.deepcopy(self.current_config)
        config_5['weights'] = {
            'base_pattern': 0.32,        # Ligera reducci√≥n
            'poi_confluence': 0.30,      # ‚≠ê Aumentar POI moderadamente
            'historical': 0.18,          # ‚≠ê Ligero aumento hist√≥rico
            'market_structure': 0.12,    # ‚≠ê Ligero aumento estructura
            'session_context': 0.08,     # Mantener sesi√≥n
        }
        config_5['confluence_distance_pips'] = 12  # Ligero aumento
        config_5['description'] = "Configuraci√≥n Balanceada Mejorada"
        optimized_configs.append(config_5)

        enviar_senal_log("INFO", f"üîß Generadas {len(optimized_configs)} configuraciones optimizadas", __name__, "confidence_calibrator")

        return optimized_configs

    def test_configuration(self, config: Dict, test_data: Optional[Dict] = None) -> Dict:
        """
        üß™ Testa una configuraci√≥n espec√≠fica
        """
        try:
            config_name = config.get('description', 'Config Sin Nombre')
            enviar_senal_log("INFO", f"üß™ Testando configuraci√≥n: {config_name}", __name__, "confidence_calibrator")

            # Aplicar configuraci√≥n temporal
            original_global_config = global_confidence_engine.config.copy()
            global_confidence_engine.update_config(config)

            # Simular c√°lculos con la nueva configuraci√≥n
            # (En implementaci√≥n real, usar√≠amos datos de patrones reales)
            simulated_scores = self._simulate_confidence_scores(config)

            test_result = {
                'config_name': config_name,
                'config': config,
                'simulated_avg_confidence': simulated_scores['avg'],
                'simulated_distribution': simulated_scores['distribution'],
                'improvement_vs_original': simulated_scores['avg'] - self.original_config.get('baseline_avg', 0.45),
                'score_variance': simulated_scores['variance'],
                'estimated_performance': self._estimate_performance(simulated_scores['avg']),
                'recommendation': self._generate_recommendation(simulated_scores),
                'timestamp': datetime.now().isoformat()
            }

            # Restaurar configuraci√≥n original
            global_confidence_engine.update_config(original_global_config)

            enviar_senal_log("INFO",
                            f"üìä Test completado: {config_name} | "
                            f"Confianza simulada: {test_result['simulated_avg_confidence']:.1%} | "
                            f"Mejora: +{test_result['improvement_vs_original']:.1%}",
                            __name__, "confidence_calibrator")

            return test_result

        except Exception as e:
            enviar_senal_log("ERROR", f"Error testando configuraci√≥n: {e}", __name__, "confidence_calibrator")
            return {'error': str(e), 'config_name': config.get('description', 'Error')}

    def _simulate_confidence_scores(self, config: Dict) -> Dict:
        """
        üé≤ Simula scores de confianza basados en la configuraci√≥n

        Algoritmo de simulaci√≥n basado en pesos y multiplicadores
        """
        try:
            weights = config.get('weights', {})
            session_multipliers = config.get('session_multipliers', {})
            confluence_distance = config.get('confluence_distance_pips', 10)

            # Factores de simulaci√≥n basados en configuraci√≥n
            base_factor = weights.get('base_pattern', 0.4) * 0.65  # Base pattern t√≠pico
            poi_factor = weights.get('poi_confluence', 0.25) * self._estimate_poi_impact(confluence_distance)
            historical_factor = weights.get('historical', 0.15) * 0.72  # Hist√≥rico promedio
            structure_factor = weights.get('market_structure', 0.10) * 0.68  # Estructura promedio
            session_factor = weights.get('session_context', 0.10) * self._estimate_session_impact(session_multipliers)

            # Score simulado combinado
            simulated_base = base_factor + poi_factor + historical_factor + structure_factor + session_factor

            # Aplicar multiplicador de sesi√≥n promedio
            avg_session_multiplier = sum(session_multipliers.values()) / len(session_multipliers)
            simulated_avg = simulated_base * avg_session_multiplier

            # Asegurar rango v√°lido
            simulated_avg = max(0.0, min(1.0, simulated_avg))

            # Calcular varianza estimada
            variance = self._calculate_config_variance(config)

            # Distribuci√≥n simulada
            distribution = self._simulate_distribution(simulated_avg, variance)

            return {
                'avg': simulated_avg,
                'variance': variance,
                'distribution': distribution,
                'factors': {
                    'base': base_factor,
                    'poi': poi_factor,
                    'historical': historical_factor,
                    'structure': structure_factor,
                    'session': session_factor
                }
            }

        except Exception as e:
            enviar_senal_log("ERROR", f"Error simulando scores: {e}", __name__, "confidence_calibrator")
            return {'avg': 0.0, 'variance': 0.0, 'distribution': {}}

    def _estimate_poi_impact(self, confluence_distance: float) -> float:
        """Estima el impacto de la confluencia POI"""
        # M√°s distancia = m√°s POIs en confluencia = mayor impacto
        base_impact = 0.75  # 75% impact base
        distance_bonus = min((confluence_distance - 10) * 0.02, 0.15)  # Max 15% bonus
        return min(1.0, base_impact + distance_bonus)

    def _estimate_session_impact(self, session_multipliers: Dict) -> float:
        """Estima el impacto promedio de sesi√≥n"""
        if not session_multipliers:
            return 0.85

        # Promedio ponderado de multiplicadores
        weighted_sum = sum(mult * weight for mult, weight in [
            (session_multipliers.get('london', 1.0), 0.30),    # Londres 30% peso
            (session_multipliers.get('new_york', 1.0), 0.25),  # NY 25% peso
            (session_multipliers.get('overlap', 1.0), 0.20),   # Overlap 20% peso
            (session_multipliers.get('asian', 1.0), 0.15),     # Asian 15% peso
            (session_multipliers.get('quiet', 1.0), 0.10),     # Quiet 10% peso
        ])

        return weighted_sum * 0.85  # Factor de calibraci√≥n

    def _calculate_config_variance(self, config: Dict) -> float:
        """Calcula la varianza esperada de la configuraci√≥n"""
        weights = config.get('weights', {})

        # Configuraciones m√°s balanceadas = menor varianza
        weight_values = list(weights.values())
        weight_std = (sum((w - 0.2) ** 2 for w in weight_values) / len(weight_values)) ** 0.5

        # Varianza base + ajuste por balance
        base_variance = 0.12
        balance_adjustment = weight_std * 0.05

        return base_variance + balance_adjustment

    def _simulate_distribution(self, avg: float, variance: float) -> Dict:
        """Simula distribuci√≥n de confianza"""
        return {
            'very_high': max(0, min(25, (avg - 0.85) * 100 + variance * 10)),
            'high': max(0, min(30, (avg - 0.70) * 50 + variance * 5)),
            'medium': max(10, min(35, 30 + variance * 20)),
            'low': max(0, min(25, (0.65 - avg) * 50 + variance * 10)),
            'very_low': max(0, min(20, (0.50 - avg) * 100 + variance * 15))
        }

    def _estimate_performance(self, simulated_avg: float) -> str:
        """Estima el grade de performance"""
        if simulated_avg >= 0.80:
            return "EXCELENTE (Meta Superada)"
        elif simulated_avg >= 0.70:
            return "MUY BUENO (Meta Alcanzada)"
        elif simulated_avg >= 0.60:
            return "BUENO (Progreso Significativo)"
        elif simulated_avg >= 0.50:
            return "REGULAR (Mejora Moderada)"
        else:
            return "BAJO (Necesita M√°s Trabajo)"

    def _generate_recommendation(self, simulated_scores: Dict) -> str:
        """Genera recomendaci√≥n basada en scores simulados"""
        avg = simulated_scores['avg']
        variance = simulated_scores['variance']

        if avg >= 0.70 and variance < 0.15:
            return "‚úÖ RECOMENDADO: Configuraci√≥n √≥ptima para Meta 70%+"
        elif avg >= 0.65 and variance < 0.20:
            return "‚ö° PROMETEDOR: Buena candidata con ligeros ajustes"
        elif avg >= 0.55:
            return "‚ö†Ô∏è MODERADO: Mejora visible pero insuficiente"
        else:
            return "‚ùå NO RECOMENDADO: Configuraci√≥n por debajo del objetivo"

    def run_calibration_sprint(self) -> Dict:
        """
        üöÄ Ejecuta el sprint completo de calibraci√≥n

        Sprint 1.6: Confidence Recalibration
        Meta: 45% ‚Üí 70%+ confianza
        """
        try:
            enviar_senal_log("INFO", "üöÄ INICIANDO SPRINT 1.6: CONFIDENCE RECALIBRATION", __name__, "confidence_calibrator")
            enviar_senal_log("INFO", "=" * 60, __name__, "confidence_calibrator")

            # 1. AN√ÅLISIS INICIAL
            enviar_senal_log("INFO", "üìä PASO 1: An√°lisis de rendimiento actual", __name__, "confidence_calibrator")
            current_performance = self.analyze_current_performance()

            # 2. GENERACI√ìN DE CONFIGURACIONES
            enviar_senal_log("INFO", "üîß PASO 2: Generaci√≥n de configuraciones optimizadas", __name__, "confidence_calibrator")
            optimized_configs = self.generate_optimized_configs()

            # 3. TESTING DE CONFIGURACIONES
            enviar_senal_log("INFO", "üß™ PASO 3: Testing de configuraciones candidatas", __name__, "confidence_calibrator")
            test_results = []

            for i, config in enumerate(optimized_configs, 1):
                enviar_senal_log("INFO", f"  üß™ Testando configuraci√≥n {i}/{len(optimized_configs)}: {config.get('description', 'Sin nombre')}", __name__, "confidence_calibrator")
                result = self.test_configuration(config)
                test_results.append(result)

            # 4. SELECCI√ìN DE MEJOR CONFIGURACI√ìN
            enviar_senal_log("INFO", "üèÜ PASO 4: Selecci√≥n de mejor configuraci√≥n", __name__, "confidence_calibrator")
            best_config, best_result = self._select_best_configuration(test_results)

            # 5. REPORTE FINAL
            sprint_report = {
                'sprint_info': {
                    'name': 'Sprint 1.6: Confidence Recalibration',
                    'objective': 'Mejorar confianza del 45% ‚Üí 70%+',
                    'timestamp': datetime.now().isoformat(),
                    'status': 'COMPLETED'
                },
                'initial_performance': current_performance,
                'tested_configurations': len(test_results),
                'test_results': test_results,
                'best_configuration': best_config,
                'best_result': best_result,
                'recommendations': self._generate_final_recommendations(best_result or {}, current_performance),
                'next_steps': self._generate_next_steps(best_result or {})
            }

            # 6. LOG DE RESULTADOS
            self._log_sprint_results(sprint_report)

            return sprint_report

        except Exception as e:
            enviar_senal_log("ERROR", f"Error en Sprint 1.6: {e}", __name__, "confidence_calibrator")
            return {'error': str(e), 'sprint_status': 'FAILED'}

    def _select_best_configuration(self, test_results: List[Dict]) -> Tuple[Optional[Dict], Optional[Dict]]:
        """Selecciona la mejor configuraci√≥n de los tests"""
        if not test_results:
            return None, None

        # Ordenar por confianza simulada promedio
        valid_results = [r for r in test_results if 'error' not in r]
        if not valid_results:
            return None, None

        best_result = max(valid_results, key=lambda x: x.get('simulated_avg_confidence', 0))
        best_config = best_result.get('config', {})

        enviar_senal_log("INFO",
                        f"üèÜ Mejor configuraci√≥n: {best_result.get('config_name', 'Unknown')} | "
                        f"Confianza: {best_result.get('simulated_avg_confidence', 0):.1%}",
                        __name__, "confidence_calibrator")

        return best_config, best_result

    def _generate_final_recommendations(self, best_result: Dict, current_performance: Dict) -> List[str]:
        """Genera recomendaciones finales"""
        recommendations = []

        if not best_result:
            recommendations.append("‚ùå No se encontr√≥ configuraci√≥n viable - revisar algoritmos base")
            return recommendations

        best_confidence = best_result.get('simulated_avg_confidence', 0)
        current_confidence = current_performance.get('avg_confidence', 0)
        improvement = best_confidence - current_confidence

        if best_confidence >= 0.70:
            recommendations.append(f"‚úÖ META ALCANZADA: Implementar '{best_result.get('config_name', 'Best Config')}' (+{improvement:.1%})")
        elif improvement >= 0.10:
            recommendations.append(f"‚ö° PROGRESO SIGNIFICATIVO: Implementar configuraci√≥n (+{improvement:.1%})")
        elif improvement >= 0.05:
            recommendations.append(f"üìà MEJORA MODERADA: Considerar implementaci√≥n (+{improvement:.1%})")
        else:
            recommendations.append("‚ö†Ô∏è MEJORA INSUFICIENTE: Revisar estrategias de calibraci√≥n")

        # Recomendaciones espec√≠ficas
        if best_result.get('config_name', '').find('POI') != -1:
            recommendations.append("üéØ ENFOQUE: Optimizar sinergia POI-ICT")

        if best_result.get('config_name', '').find('Sesi√≥n') != -1:
            recommendations.append("‚è∞ ENFOQUE: Optimizar multiplicadores de sesi√≥n")

        if best_result.get('config_name', '').find('Agresiv') != -1:
            recommendations.append("üöÄ ENFOQUE: Configuraci√≥n agresiva - monitorear estabilidad")

        return recommendations

    def _generate_next_steps(self, best_result: Dict) -> List[str]:
        """Genera pr√≥ximos pasos"""
        next_steps = []

        if not best_result:
            next_steps.append("1. Revisar algoritmos base del motor de confianza")
            next_steps.append("2. Validar datos de entrada y POIs")
            next_steps.append("3. Considerar refactoring del sistema de scoring")
            return next_steps

        best_confidence = best_result.get('simulated_avg_confidence', 0)

        if best_confidence >= 0.70:
            next_steps.append("1. ‚úÖ Implementar configuraci√≥n √≥ptima en producci√≥n")
            next_steps.append("2. üìä Monitorear rendimiento en tiempo real")
            next_steps.append("3. üéØ Proceder con Sprint 1.7: Advanced Patterns")
        elif best_confidence >= 0.60:
            next_steps.append("1. ‚ö° Implementar mejor configuraci√≥n encontrada")
            next_steps.append("2. üîß Continuar optimizaci√≥n fine-tuning")
            next_steps.append("3. üìà Validar con datos hist√≥ricos")
        else:
            next_steps.append("1. ‚ö†Ô∏è Revisar algoritmos fundamentales")
            next_steps.append("2. üß™ Probar configuraciones m√°s agresivas")
            next_steps.append("3. üîç An√°lisis profundo de factores limitantes")

        next_steps.append("4. üìù Documentar cambios en bit√°cora t√©cnica")
        next_steps.append("5. üß™ Preparar tests de regresi√≥n")

        return next_steps

    def _log_sprint_results(self, sprint_report: Dict) -> None:
        """Log detallado de resultados del sprint"""
        enviar_senal_log("INFO", "", __name__, "confidence_calibrator")
        enviar_senal_log("INFO", "üèÅ SPRINT 1.6 COMPLETADO - REPORTE FINAL", __name__, "confidence_calibrator")
        enviar_senal_log("INFO", "=" * 60, __name__, "confidence_calibrator")

        # Performance inicial vs mejor resultado
        initial_conf = sprint_report.get('initial_performance', {}).get('avg_confidence', 0)
        best_conf = sprint_report.get('best_result', {}).get('simulated_avg_confidence', 0)
        improvement = best_conf - initial_conf

        enviar_senal_log("INFO", f"üìä RENDIMIENTO INICIAL: {initial_conf:.1%}", __name__, "confidence_calibrator")
        enviar_senal_log("INFO", f"üéØ MEJOR RESULTADO: {best_conf:.1%}", __name__, "confidence_calibrator")
        enviar_senal_log("INFO", f"üìà MEJORA TOTAL: +{improvement:.1%}", __name__, "confidence_calibrator")

        # Meta del sprint
        meta_alcanzada = best_conf >= 0.70
        enviar_senal_log("INFO", f"üèÜ META 70%+: {'‚úÖ ALCANZADA' if meta_alcanzada else '‚ùå NO ALCANZADA'}", __name__, "confidence_calibrator")

        # Mejor configuraci√≥n
        best_config_name = sprint_report.get('best_result', {}).get('config_name', 'N/A')
        enviar_senal_log("INFO", f"üèÜ MEJOR CONFIG: {best_config_name}", __name__, "confidence_calibrator")

        # Recomendaciones
        recommendations = sprint_report.get('recommendations', [])
        enviar_senal_log("INFO", "üí° RECOMENDACIONES:", __name__, "confidence_calibrator")
        for i, rec in enumerate(recommendations, 1):
            enviar_senal_log("INFO", f"  {i}. {rec}", __name__, "confidence_calibrator")

        # Pr√≥ximos pasos
        next_steps = sprint_report.get('next_steps', [])
        enviar_senal_log("INFO", "üöÄ PR√ìXIMOS PASOS:", __name__, "confidence_calibrator")
        for i, step in enumerate(next_steps, 1):
            enviar_senal_log("INFO", f"  {i}. {step}", __name__, "confidence_calibrator")

        enviar_senal_log("INFO", "=" * 60, __name__, "confidence_calibrator")

    def apply_best_configuration(self, sprint_report: Dict) -> bool:
        """
        ‚úÖ Aplica la mejor configuraci√≥n encontrada al motor global
        """
        try:
            best_config = sprint_report.get('best_configuration')
            if not best_config:
                enviar_senal_log("ERROR", "No hay configuraci√≥n v√°lida para aplicar", __name__, "confidence_calibrator")
                return False

            config_name = sprint_report.get('best_result', {}).get('config_name', 'Best Config')

            enviar_senal_log("INFO", f"‚öôÔ∏è Aplicando configuraci√≥n √≥ptima: {config_name}", __name__, "confidence_calibrator")

            # Aplicar al motor global
            success = global_confidence_engine.update_config(best_config)

            if success:
                enviar_senal_log("SUCCESS", f"‚úÖ Configuraci√≥n '{config_name}' aplicada exitosamente", __name__, "confidence_calibrator")
                enviar_senal_log("INFO", "üéØ Motor de confianza recalibrado - Sprint 1.6 implementado", __name__, "confidence_calibrator")
                return True
            else:
                enviar_senal_log("ERROR", "‚ùå Error aplicando configuraci√≥n al motor", __name__, "confidence_calibrator")
                return False

        except Exception as e:
            enviar_senal_log("ERROR", f"Error aplicando configuraci√≥n: {e}", __name__, "confidence_calibrator")
            return False


def main():
    """Funci√≥n principal para testing del calibrador"""
    enviar_senal_log("INFO", "üéØ CONFIDENCE CALIBRATOR - Sprint 1.6", __name__, "confidence_calibrator")
    enviar_senal_log("INFO", "Meta: Mejorar confianza del 45% ‚Üí 70%+", __name__, "confidence_calibrator")
    enviar_senal_log("INFO", "=" * 60, __name__, "confidence_calibrator")

    # Inicializar calibrador
    calibrator = ConfidenceCalibrator()

    # Ejecutar sprint completo
    sprint_report = calibrator.run_calibration_sprint()

    # Verificar resultados
    if 'error' not in sprint_report:
        enviar_senal_log("SUCCESS", "‚úÖ Sprint 1.6 completado exitosamente", __name__, "confidence_calibrator")

        # Preguntar si aplicar la mejor configuraci√≥n
        best_result = sprint_report.get('best_result', {})
        if best_result.get('simulated_avg_confidence', 0) > 0.60:
            enviar_senal_log("INFO", "üéØ Configuraci√≥n prometedora encontrada - lista para implementaci√≥n", __name__, "confidence_calibrator")

            # En un entorno real, aqu√≠ se aplicar√≠a la configuraci√≥n
            # calibrator.apply_best_configuration(sprint_report)
        else:
            enviar_senal_log("WARNING", "‚ö†Ô∏è Resultados por debajo de expectativas - necesaria m√°s optimizaci√≥n", __name__, "confidence_calibrator")
    else:
        enviar_senal_log("ERROR", f"‚ùå Sprint 1.6 fall√≥: {sprint_report.get('error', 'Error desconocido')}", __name__, "confidence_calibrator")


if __name__ == "__main__":
    main()
