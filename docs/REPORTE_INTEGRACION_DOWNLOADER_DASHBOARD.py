#!/usr/bin/env python3
"""
üìä REPORTE DE INTEGRACI√ìN: ADVANCED CANDLE DOWNLOADER ‚Üî DASHBOARD
===============================================================================

OBJETIVO: Conectar el Advanced Candle Downloader con el dashboard para
automatizar la descarga y actualizaci√≥n de datos que alimenten a todos
los sistemas (POI, ICT, Fractal, etc.)

AN√ÅLISIS DEL FLUJO ACTUAL:
=========================

1. üöÄ PUNTO DE ENTRADA - main.py ‚Üí dashboard_definitivo.py
   - El usuario ejecuta: python main.py --dashboard
   - Se lanza dashboard_definitivo.py como interfaz principal
   - El dashboard se conecta a MT5 usando get_mt5_manager()

2. üìä FLUJO DE DATOS ACTUAL (dashboard_definitivo.py):
   L√≠nea 447: self.mt5_manager = get_mt5_manager()
   L√≠nea 1086: data = self.mt5_manager.get_historical_data(symbol, tf_code, bars)

   LIMITACI√ìN DETECTADA:
   - Solo descarga cantidades peque√±as (200-300 velas)
   - No hay persistencia a largo plazo de datos
   - Cada arranque requiere re-descarga

3. üéØ SISTEMAS QUE CONSUMEN DATOS:
   a) ICT Engine (core/ict_engine/):
      - ict_detector.py - An√°lisis de patrones ICT
      - fractal_analyzer.py - An√°lisis de fractales
      - confidence_engine.py - Motor de confianza

   b) POI System (core/poi_system/):
      - poi_detector.py - Detecci√≥n de POIs
      - poi_scoring_engine.py - Scoring de POIs

   c) Trading Engine (core/trading.py):
      - An√°lisis de trade execution

PROPUESTA DE INTEGRACI√ìN:
========================

FASE 1: MODIFICACI√ìN DEL DASHBOARD (dashboard_definitivo.py)
----------------------------------------------------------

A. AGREGAR ADVANCED CANDLE DOWNLOADER AL INICIO

   L√≠nea ~77 (imports):
   + from utils.advanced_candle_downloader import AdvancedCandleDownloader

   L√≠nea ~315 (inicializaci√≥n):
   + self.candle_downloader = AdvancedCandleDownloader()

B. NUEVA FUNCI√ìN: auto_download_data_on_startup()

   def auto_download_data_on_startup(self):
       \"\"\"Descarga autom√°tica de datos al iniciar el dashboard\"\"\"
       try:
           # Verificar si necesitamos datos frescos
           if self._needs_data_refresh():
               enviar_senal_log("INFO", "üîÑ Descarga autom√°tica iniciada", __name__, "downloader")

               # Descarga optimizada para el dashboard
               stats = self.candle_downloader.download_multiple(
                   symbols=["EURUSD"],  # Symbol principal
                   timeframes=["M1", "M5", "H1", "H4"],  # TFs cr√≠ticos
                   lookback=100000,  # 100K velas (suficiente para an√°lisis)
                   use_parallel=True
               )

               enviar_senal_log("INFO", f"‚úÖ Descarga completada: {len(stats)} timeframes", __name__, "downloader")
               return True
       except Exception as e:
           enviar_senal_log("ERROR", f"‚ùå Error en descarga autom√°tica: {e}", __name__, "downloader")
           return False

C. FUNCI√ìN DE VERIFICACI√ìN: _needs_data_refresh()

   def _needs_data_refresh(self) -> bool:
       \"\"\"Verifica si necesitamos refrescar los datos\"\"\"
       try:
           # Verificar archivos existentes
           data_dir = Path("data/candles")
           if not data_dir.exists():
               return True

           # Verificar timestamps de archivos cr√≠ticos
           critical_files = ["M1.csv", "H1.csv", "H4.csv"]
           for file in critical_files:
               file_path = data_dir / file
               if not file_path.exists():
                   return True

               # Verificar si el archivo es muy viejo (>24 horas)
               file_age = time.time() - file_path.stat().st_mtime
               if file_age > 86400:  # 24 horas
                   return True

           return False
       except Exception:
           return True  # Si hay error, mejor descargar

D. MODIFICAR load_multi_timeframe_data() (l√≠nea ~1060)

   def load_multi_timeframe_data(self):
       \"\"\"Carga datos optimizada usando archivos pre-descargados\"\"\"
       try:
           # üîÑ NUEVO: Primero intentar cargar desde archivos CSV
           if self._load_from_csv_files():
               enviar_senal_log("INFO", "üìÇ Datos cargados desde archivos CSV", __name__, "data")
               return

           # üîÑ FALLBACK: Si no hay archivos, usar MT5 directo (m√©todo actual)
           if not self.mt5_manager:
               enviar_senal_log("WARNING", "üî¥ MT5 Manager no disponible", __name__, "data")
               return

           # C√≥digo actual de carga desde MT5...
           [mantener c√≥digo existente como fallback]

FASE 2: INTEGRACI√ìN CON SISTEMAS CONSUMIDORES
---------------------------------------------

A. ICT ENGINE (core/ict_engine/ict_detector.py)

   Modificar para usar datos pre-descargados:

   def load_market_data(self, symbol: str, timeframe: str) -> pd.DataFrame:
       \"\"\"Carga datos desde archivos pre-descargados\"\"\"
       try:
           data_file = Path(f"data/candles/{timeframe}.csv")
           if data_file.exists():
               df = pd.read_csv(data_file)
               df['time'] = pd.to_datetime(df['time'])
               return df.tail(1000)  # √öltimas 1000 velas para an√°lisis
           else:
               # Fallback a MT5 directo
               return self._load_from_mt5(symbol, timeframe)
       except Exception as e:
           enviar_senal_log("ERROR", f"Error cargando datos: {e}", __name__, "ict")
           return pd.DataFrame()

B. FRACTAL ANALYZER (core/ict_engine/fractal_analyzer.py)

   Similar integraci√≥n para usar archivos pre-descargados.

C. POI SYSTEM (core/poi_system/poi_detector.py)

   Usar el mismo patr√≥n de carga desde archivos CSV.

FASE 3: SISTEMA DE ACTUALIZACI√ìN AUTOM√ÅTICA
-------------------------------------------

A. AGREGAR BINDING AL DASHBOARD (l√≠nea ~350)

   Binding("ctrl+u", "update_data", "Actualizar Datos"),

B. FUNCI√ìN DE ACTUALIZACI√ìN MANUAL:

   def action_update_data(self):
       \"\"\"Actualizaci√≥n manual de datos v√≠a hotkey\"\"\"
       self.notify("üîÑ Iniciando actualizaci√≥n de datos...")

       # Ejecutar descarga en background
       import threading
       thread = threading.Thread(target=self._background_data_update)
       thread.daemon = True
       thread.start()

   def _background_data_update(self):
       \"\"\"Actualizaci√≥n de datos en background\"\"\"
       try:
           stats = self.candle_downloader.download_multiple(
               symbols=["EURUSD"],
               timeframes=["M1", "M5", "H1", "H4"],
               lookback=50000,  # Actualizaci√≥n m√°s r√°pida
               use_parallel=True
           )

           # Notificar al usuario
           self.call_from_thread(self.notify, f"‚úÖ Datos actualizados: {len(stats)} timeframes")

           # Recargar datos en dashboard
           self.call_from_thread(self._reload_dashboard_data)

       except Exception as e:
           self.call_from_thread(self.notify, f"‚ùå Error actualizando: {e}")

FASE 4: CONFIGURACI√ìN Y OPTIMIZACI√ìN
------------------------------------

A. ARCHIVO DE CONFIGURACI√ìN (config/data_config.py):

   DATA_UPDATE_CONFIG = {
       "auto_download_on_startup": True,
       "symbols": ["EURUSD"],
       "timeframes": ["M1", "M5", "H1", "H4"],
       "lookback_startup": 100000,
       "lookback_update": 50000,
       "max_file_age_hours": 24,
       "parallel_downloads": True,
       "verify_integrity": True,
       "backup_old_data": True
   }

B. SISTEMA DE LOGS DEDICADO:

   Usar smart_directory_logger.py para logs espec√≠ficos de descarga:
   - data/logs/mt5/ para logs de conexi√≥n MT5
   - data/logs/daily/ para logs de descarga diaria

IMPLEMENTACI√ìN PASO A PASO:
===========================

PASO 1: Modificar dashboard_definitivo.py
- Agregar imports
- Agregar inicializaci√≥n del downloader
- Implementar auto_download_data_on_startup()
- Modificar load_multi_timeframe_data()

PASO 2: Crear funciones auxiliares
- _needs_data_refresh()
- _load_from_csv_files()
- _background_data_update()

PASO 3: Integrar sistemas consumidores
- Modificar ICT Engine para usar CSV
- Modificar POI System para usar CSV
- Modificar Fractal Analyzer para usar CSV

PASO 4: Testing y validaci√≥n
- Test de flujo completo
- Verificar rendimiento
- Validar integridad de datos

BENEFICIOS ESPERADOS:
====================

1. üöÄ ARRANQUE M√ÅS R√ÅPIDO: Dashboard carga desde archivos pre-descargados
2. üìä DATOS CONSISTENTES: Todos los sistemas usan la misma fuente
3. üîÑ ACTUALIZACI√ìN AUTOM√ÅTICA: Datos frescos sin intervenci√≥n manual
4. üíæ PERSISTENCIA: Datos guardados para an√°lisis offline
5. ‚ö° MEJOR RENDIMIENTO: Menos llamadas a MT5 en tiempo real
6. üõ°Ô∏è MAYOR ROBUSTEZ: Fallbacks en caso de problemas MT5

CRONOGRAMA ESTIMADO:
===================

- Fase 1: 2-3 horas (modificaci√≥n dashboard)
- Fase 2: 2-3 horas (integraci√≥n sistemas)
- Fase 3: 1-2 horas (sistema actualizaci√≥n)
- Fase 4: 1 hora (configuraci√≥n)
- Testing: 1-2 horas

TOTAL: 7-11 horas de desarrollo

RIESGOS Y MITIGACIONES:
======================

RIESGO: Conflictos con c√≥digo existente
MITIGACI√ìN: Implementar como funcionalidad adicional, mantener fallbacks

RIESGO: Problemas de rendimiento
MITIGACI√ìN: Carga paralela, datos en chunks, validaci√≥n de memoria

RIESGO: Datos corruptos
MITIGACI√ìN: Validaci√≥n con pandas/numpy, backups autom√°ticos

PR√ìXIMOS PASOS:
===============

1. ‚úÖ Preparar este reporte (COMPLETADO)
2. üîÑ Implementar Fase 1 (modificar dashboard)
3. üîÑ Testing inicial
4. üîÑ Implementar Fases 2-4
5. üîÑ Testing completo y validaci√≥n

¬øPROCEDER CON LA IMPLEMENTACI√ìN?
"""

print(__doc__)
