#!/usr/bin/env python3
"""
ğŸ›ï¸ ACC FLOW CONTROLLER - Controlador de flujo avanzado del ACC
ARQUITECTURA: GestiÃ³n inteligente de flujos, caching y optimizaciÃ³n
PROTOCOLO: "Caja Negra" - Control fino de ejecuciÃ³n, logging exhaustivo
"""

import asyncio
import time
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
from collections import deque, defaultdict
from dataclasses import dataclass
from enum import Enum

# ğŸ”Œ IMPORTS DEL ICT ENGINE
from sistema.logging_interface import enviar_senal_log

# ğŸ§  IMPORTS DEL ACC
from .acc_data_models import (
    AnalysisInput,
    AnalysisOutput,
    AnalysisStatus,
    ComponentType
)

class FlowPriority(Enum):
    """Prioridades de flujo de anÃ¡lisis"""
    URGENT = "URGENT"       # AnÃ¡lisis crÃ­ticos
    HIGH = "HIGH"           # AnÃ¡lisis de alta prioridad  
    NORMAL = "NORMAL"       # AnÃ¡lisis estÃ¡ndar
    LOW = "LOW"             # AnÃ¡lisis de baja prioridad
    BACKGROUND = "BACKGROUND"  # AnÃ¡lisis en background

class CacheStrategy(Enum):
    """Estrategias de caching"""
    NO_CACHE = "NO_CACHE"           # Sin cache
    AGGRESSIVE = "AGGRESSIVE"       # Cache agresivo
    CONSERVATIVE = "CONSERVATIVE"   # Cache conservador
    INTELLIGENT = "INTELLIGENT"     # Cache inteligente

@dataclass
class FlowMetrics:
    """MÃ©tricas de flujo y performance"""
    
    total_analyses: int = 0
    successful_analyses: int = 0
    failed_analyses: int = 0
    avg_execution_time_ms: float = 0.0
    cache_hit_rate: float = 0.0
    throughput_per_minute: float = 0.0
    queue_length: int = 0
    
    def get_success_rate(self) -> float:
        """Calcular tasa de Ã©xito"""
        if self.total_analyses == 0:
            return 0.0
        return self.successful_analyses / self.total_analyses
    
    def get_summary(self) -> Dict[str, Any]:
        """Resumen de mÃ©tricas"""
        return {
            "total": self.total_analyses,
            "success_rate": f"{self.get_success_rate():.1%}",
            "avg_time_ms": f"{self.avg_execution_time_ms:.0f}",
            "cache_hit_rate": f"{self.cache_hit_rate:.1%}",
            "throughput_min": f"{self.throughput_per_minute:.1f}",
            "queue_length": self.queue_length
        }

class AccFlowController:
    """
    ğŸ›ï¸ Controlador de flujo avanzado del ACC
    
    RESPONSABILIDADES:
    - GestiÃ³n de colas de anÃ¡lisis por prioridad
    - Caching inteligente de resultados
    - OptimizaciÃ³n de flujos de ejecuciÃ³n
    - Control de concurrencia y throttling
    - MÃ©tricas avanzadas de performance
    """
    
    def __init__(self,
                 max_concurrent_analyses: int = 3,
                 cache_ttl_minutes: int = 5,
                 cache_strategy: CacheStrategy = CacheStrategy.INTELLIGENT,
                 enable_flow_optimization: bool = True):
        """
        ğŸ›ï¸ InicializaciÃ³n del controlador de flujo
        
        Args:
            max_concurrent_analyses: MÃ¡ximo anÃ¡lisis concurrentes
            cache_ttl_minutes: TTL del cache en minutos
            cache_strategy: Estrategia de caching
            enable_flow_optimization: Activar optimizaciones de flujo
        """
        
        # âš™ï¸ CONFIGURACIÃ“N
        self.max_concurrent_analyses = max_concurrent_analyses
        self.cache_ttl_minutes = cache_ttl_minutes
        self.cache_strategy = cache_strategy
        self.enable_flow_optimization = enable_flow_optimization
        
        # ğŸ“Š COLAS DE PRIORIDAD
        self.priority_queues = {
            FlowPriority.URGENT: deque(),
            FlowPriority.HIGH: deque(),
            FlowPriority.NORMAL: deque(),
            FlowPriority.LOW: deque(),
            FlowPriority.BACKGROUND: deque()
        }
        
        # ğŸ’¾ SISTEMA DE CACHE
        self.results_cache = {}  # cache_key -> (result, timestamp)
        self.cache_stats = defaultdict(int)
        
        # ğŸ“Š MÃ‰TRICAS Y ESTADO
        self.flow_metrics = FlowMetrics()
        self.active_analyses = {}  # analysis_id -> start_time
        self.execution_history = deque(maxlen=1000)  # Historial limitado
        
        # ğŸ¯ OPTIMIZACIONES
        self.flow_patterns = {}  # symbol -> pattern_data
        self.component_performance = defaultdict(list)  # component -> [times]
        
        # ğŸ“ LOG INICIALIZACIÃ“N
        enviar_senal_log(
            'DEBUG',
            f"AccFlowController inicializado | "
                   f"Max Concurrent: {max_concurrent_analyses} | "
                   f"Cache TTL: {cache_ttl_minutes}min | "
                   f"Strategy: {cache_strategy.value}",
            'acc_flow_controller',
            'acc'
        )
    
    def should_use_cache(self, analysis_input: AnalysisInput) -> bool:
        """
        ğŸ’¾ Determinar si usar cache para un anÃ¡lisis
        
        Args:
            analysis_input: ParÃ¡metros de anÃ¡lisis
            
        Returns:
            bool: True si debe usar cache
        """
        
        if self.cache_strategy == CacheStrategy.NO_CACHE:
            return False
        
        # ğŸ”‘ GENERAR CACHE KEY
        cache_key = self._generate_cache_key(analysis_input)
        
        # âœ… VERIFICAR EXISTENCIA Y VALIDEZ
        if cache_key in self.results_cache:
            result, timestamp = self.results_cache[cache_key]
            
            # â° VERIFICAR TTL
            age_minutes = (datetime.now() - timestamp).total_seconds() / 60
            
            if age_minutes <= self.cache_ttl_minutes:
                # ğŸ“Š REGISTRAR HIT
                self.cache_stats['hits'] += 1
                self._update_cache_hit_rate()
                
                enviar_senal_log(
                    level='DEBUG',
                    message=f"ğŸ’¾ Cache HIT | Key: {cache_key[:20]}... | Age: {age_minutes:.1f}min",
                    emisor='acc_flow_controller',
                    categoria='acc'
                )
                
                return True
            else:
                # ğŸ—‘ï¸ CACHE EXPIRADO
                del self.results_cache[cache_key]
                enviar_senal_log(
                    level='DEBUG',
                    message=f"ğŸ’¾ Cache EXPIRED | Key: {cache_key[:20]}... | Age: {age_minutes:.1f}min",
                    emisor='acc_flow_controller',
                    categoria='acc'
                )
        
        # ğŸ“Š REGISTRAR MISS
        self.cache_stats['misses'] += 1
        self._update_cache_hit_rate()
        
        return False
    
    def get_cached_result(self, analysis_input: AnalysisInput) -> Optional[AnalysisOutput]:
        """
        ğŸ’¾ Obtener resultado del cache
        
        Args:
            analysis_input: ParÃ¡metros de anÃ¡lisis
            
        Returns:
            AnalysisOutput o None si no estÃ¡ en cache
        """
        
        cache_key = self._generate_cache_key(analysis_input)
        
        if cache_key in self.results_cache:
            result, timestamp = self.results_cache[cache_key]
            
            # ğŸ“Š CREAR COPIA CON NUEVO ID
            cached_result = self._create_cached_copy(result, analysis_input)
            
            enviar_senal_log(
                level='DEBUG',
                message=f"ğŸ’¾ Cache result returned | Original ID: {result.analysis_id} | "
                       f"New ID: {cached_result.analysis_id}",
                emisor='acc_flow_controller',
                categoria='acc'
            )
            
            return cached_result
        
        return None
    
    def cache_result(self, analysis_input: AnalysisInput, result: AnalysisOutput):
        """
        ğŸ’¾ Almacenar resultado en cache
        
        Args:
            analysis_input: ParÃ¡metros de anÃ¡lisis
            result: Resultado a cachear
        """
        
        if self.cache_strategy == CacheStrategy.NO_CACHE:
            return
        
        cache_key = self._generate_cache_key(analysis_input)
        self.results_cache[cache_key] = (result, datetime.now())
        
        enviar_senal_log(
            level='DEBUG',
            message=f"ğŸ’¾ Result cached | Key: {cache_key[:20]}... | ID: {result.analysis_id}",
            emisor='acc_flow_controller',
            categoria='acc'
        )
        
        # ğŸ§¹ CLEANUP PERIÃ“DICO
        self._cleanup_expired_cache()
    
    def queue_analysis(self, 
                      analysis_input: AnalysisInput, 
                      executor_func: Callable,
                      priority: FlowPriority = FlowPriority.NORMAL) -> str:
        """
        ğŸ“¥ Encolar anÃ¡lisis para ejecuciÃ³n
        
        Args:
            analysis_input: ParÃ¡metros de anÃ¡lisis
            executor_func: FunciÃ³n ejecutora del anÃ¡lisis
            priority: Prioridad de ejecuciÃ³n
            
        Returns:
            str: ID del anÃ¡lisis encolado
        """
        
        # ğŸ“¦ CREAR ITEM DE COLA
        queue_item = {
            'analysis_input': analysis_input,
            'executor_func': executor_func,
            'queue_timestamp': datetime.now(),
            'priority': priority
        }
        
        # ğŸ“¥ AÃ‘ADIR A COLA APROPIADA
        self.priority_queues[priority].append(queue_item)
        
        # ğŸ“Š ACTUALIZAR MÃ‰TRICAS
        self.flow_metrics.queue_length = sum(len(queue) for queue in self.priority_queues.values())
        
        enviar_senal_log(
            level='DEBUG',
            message=f"ğŸ“¥ Analysis queued | ID: {analysis_input.analysis_id} | "
                   f"Priority: {priority.value} | Queue Length: {self.flow_metrics.queue_length}",
            emisor='acc_flow_controller',
            categoria='acc'
        )
        
        return analysis_input.analysis_id
    
    def get_next_analysis(self) -> Optional[Dict[str, Any]]:
        """
        ğŸ“¤ Obtener prÃ³ximo anÃ¡lisis de las colas por prioridad
        
        Returns:
            Dict con anÃ¡lisis a ejecutar o None si no hay ninguno
        """
        
        # ğŸ¯ REVISAR COLAS POR PRIORIDAD
        for priority in FlowPriority:
            queue = self.priority_queues[priority]
            
            if queue:
                # ğŸ“¤ EXTRAER PRIMER ELEMENTO
                analysis_item = queue.popleft()
                
                # ğŸ“Š ACTUALIZAR MÃ‰TRICAS
                self.flow_metrics.queue_length = sum(len(q) for q in self.priority_queues.values())
                
                enviar_senal_log(
                    level='DEBUG',
                    message=f"ğŸ“¤ Analysis dequeued | ID: {analysis_item['analysis_input'].analysis_id} | "
                           f"Priority: {priority.value} | Remaining: {self.flow_metrics.queue_length}",
                    emisor='acc_flow_controller',
                    categoria='acc'
                )
                
                return analysis_item
        
        return None
    
    def can_execute_analysis(self) -> bool:
        """
        ğŸš¦ Verificar si se puede ejecutar un nuevo anÃ¡lisis
        
        Returns:
            bool: True si se puede ejecutar
        """
        
        current_active = len(self.active_analyses)
        can_execute = current_active < self.max_concurrent_analyses
        
        if not can_execute:
            enviar_senal_log(
                level='DEBUG',
                message=f"ğŸš¦ Execution blocked | Active: {current_active} | Max: {self.max_concurrent_analyses}",
                emisor='acc_flow_controller',
                categoria='acc'
            )
        
        return can_execute
    
    def register_analysis_start(self, analysis_id: str):
        """
        ğŸ“Š Registrar inicio de anÃ¡lisis
        
        Args:
            analysis_id: ID del anÃ¡lisis
        """
        
        self.active_analyses[analysis_id] = datetime.now()
        
        enviar_senal_log(
            level='DEBUG',
            message=f"ğŸ“Š Analysis started | ID: {analysis_id} | Active: {len(self.active_analyses)}",
            emisor='acc_flow_controller',
            categoria='acc'
        )
    
    def register_analysis_completion(self, 
                                   analysis_id: str, 
                                   result: AnalysisOutput, 
                                   success: bool = True):
        """
        ğŸ“Š Registrar finalizaciÃ³n de anÃ¡lisis
        
        Args:
            analysis_id: ID del anÃ¡lisis
            result: Resultado del anÃ¡lisis
            success: Si fue exitoso
        """
        
        # â±ï¸ CALCULAR TIEMPO DE EJECUCIÃ“N
        start_time = self.active_analyses.get(analysis_id)
        execution_time = 0.0
        
        if start_time:
            execution_time = (datetime.now() - start_time).total_seconds() * 1000
            del self.active_analyses[analysis_id]
        
        # ğŸ“Š ACTUALIZAR MÃ‰TRICAS
        self.flow_metrics.total_analyses += 1
        
        if success:
            self.flow_metrics.successful_analyses += 1
        else:
            self.flow_metrics.failed_analyses += 1
        
        # ğŸ“ˆ ACTUALIZAR TIEMPO PROMEDIO
        total_time = (self.flow_metrics.avg_execution_time_ms * (self.flow_metrics.total_analyses - 1) + execution_time)
        self.flow_metrics.avg_execution_time_ms = total_time / self.flow_metrics.total_analyses
        
        # ğŸ“š AÃ‘ADIR AL HISTORIAL
        self.execution_history.append({
            'analysis_id': analysis_id,
            'execution_time_ms': execution_time,
            'success': success,
            'timestamp': datetime.now()
        })
        
        # ğŸ¯ ACTUALIZAR THROUGHPUT
        self._update_throughput()
        
        enviar_senal_log(
            level='DEBUG',
            message=f"ğŸ“Š Analysis completed | ID: {analysis_id} | "
                   f"Time: {execution_time:.0f}ms | Success: {success} | "
                   f"Success Rate: {self.flow_metrics.get_success_rate():.1%}",
            emisor='acc_flow_controller',
            categoria='acc'
        )
    
    def get_flow_metrics(self) -> Dict[str, Any]:
        """
        ğŸ“Š Obtener mÃ©tricas de flujo actuales
        
        Returns:
            Dict con mÃ©tricas completas
        """
        
        return {
            "flow_metrics": self.flow_metrics.get_summary(),
            "cache_stats": dict(self.cache_stats),
            "active_analyses": len(self.active_analyses),
            "queue_summary": {
                priority.value: len(queue) 
                for priority, queue in self.priority_queues.items()
            },
            "cache_size": len(self.results_cache),
            "history_size": len(self.execution_history)
        }
    
    def optimize_flow(self, symbol: str) -> Dict[str, Any]:
        """
        ğŸ¯ Optimizar flujo para sÃ­mbolo especÃ­fico
        
        Args:
            symbol: SÃ­mbolo a optimizar
            
        Returns:
            Dict con recomendaciones de optimizaciÃ³n
        """
        
        if not self.enable_flow_optimization:
            return {"optimization": "disabled"}
        
        # ğŸ“Š ANALIZAR PATRONES HISTÃ“RICOS
        symbol_history = [
            entry for entry in self.execution_history 
            if entry.get('symbol') == symbol
        ]
        
        if len(symbol_history) < 5:
            return {"optimization": "insufficient_data"}
        
        # ğŸ“ˆ CALCULAR MÃ‰TRICAS
        avg_time = sum(entry['execution_time_ms'] for entry in symbol_history) / len(symbol_history)
        success_rate = sum(1 for entry in symbol_history if entry['success']) / len(symbol_history)
        
        # ğŸ¯ GENERAR RECOMENDACIONES
        recommendations = []
        
        if avg_time > self.flow_metrics.avg_execution_time_ms * 1.2:
            recommendations.append("consider_increased_cache_ttl")
        
        if success_rate < 0.8:
            recommendations.append("review_component_configuration")
        
        optimization_data = {
            "symbol": symbol,
            "avg_execution_time_ms": avg_time,
            "success_rate": success_rate,
            "sample_size": len(symbol_history),
            "recommendations": recommendations,
            "suggested_priority": FlowPriority.HIGH.value if success_rate > 0.9 else FlowPriority.NORMAL.value
        }
        
        # ğŸ’¾ ALMACENAR PATRÃ“N
        self.flow_patterns[symbol] = optimization_data
        
        enviar_senal_log(
            level='DEBUG',
            message=f"ğŸ¯ Flow optimized | Symbol: {symbol} | "
                   f"Avg Time: {avg_time:.0f}ms | Success: {success_rate:.1%}",
            emisor='acc_flow_controller',
            categoria='acc'
        )
        
        return optimization_data
    
    def _generate_cache_key(self, analysis_input: AnalysisInput) -> str:
        """ğŸ”‘ Generar clave de cache para anÃ¡lisis"""
        
        # ğŸ¯ COMPONENTES DE LA CLAVE
        key_components = [
            analysis_input.symbol,
            "_".join(sorted(analysis_input.timeframes)),
            analysis_input.analysis_type,
            str(analysis_input.confidence_threshold),
            str(analysis_input.poi_limit)
        ]
        
        # ğŸ” GENERAR HASH
        cache_key = "_".join(key_components)
        return cache_key
    
    def _create_cached_copy(self, original_result: AnalysisOutput, new_input: AnalysisInput) -> AnalysisOutput:
        """ğŸ“Š Crear copia de resultado cacheado con nuevo ID"""
        
        # ğŸ”„ CLONAR RESULTADO (simplificado)
        cached_result = AnalysisOutput(
            analysis_id=new_input.analysis_id,
            input_parameters=new_input,
            analysis_status=AnalysisStatus.COMPLETED,
            completion_timestamp=datetime.now().isoformat()
        )
        
        # ğŸ“Š COPIAR DATOS PRINCIPALES
        cached_result.market_structure = original_result.market_structure
        cached_result.poi_data = original_result.poi_data
        cached_result.confidence_data = original_result.confidence_data
        cached_result.veredicto_data = original_result.veredicto_data
        cached_result.tct_data = original_result.tct_data
        
        # ğŸ“Š COPIAR MÃ‰TRICAS (con ajustes)
        cached_result.overall_success_rate = original_result.overall_success_rate
        cached_result.total_execution_time_ms = 0.0  # Cache hit = tiempo mÃ­nimo
        cached_result.analysis_quality_score = original_result.analysis_quality_score
        
        return cached_result
    
    def _update_cache_hit_rate(self):
        """ğŸ“Š Actualizar tasa de hit del cache"""
        
        hits = self.cache_stats['hits']
        misses = self.cache_stats['misses']
        total = hits + misses
        
        if total > 0:
            self.flow_metrics.cache_hit_rate = hits / total
    
    def _update_throughput(self):
        """ğŸ“ˆ Actualizar throughput por minuto"""
        
        # ğŸ“Š CONTAR ANÃLISIS EN LA ÃšLTIMA HORA
        cutoff_time = datetime.now() - timedelta(minutes=60)
        recent_analyses = [
            entry for entry in self.execution_history 
            if entry['timestamp'] > cutoff_time
        ]
        
        # ğŸ“ˆ CALCULAR THROUGHPUT
        if recent_analyses:
            time_span_minutes = (datetime.now() - recent_analyses[0]['timestamp']).total_seconds() / 60
            if time_span_minutes > 0:
                self.flow_metrics.throughput_per_minute = len(recent_analyses) / time_span_minutes
    
    def _cleanup_expired_cache(self):
        """ğŸ§¹ Limpiar entradas expiradas del cache"""
        
        if len(self.results_cache) < 100:  # Solo limpiar si hay muchas entradas
            return
        
        cutoff_time = datetime.now() - timedelta(minutes=self.cache_ttl_minutes)
        expired_keys = []
        
        for cache_key, (result, timestamp) in self.results_cache.items():
            if timestamp < cutoff_time:
                expired_keys.append(cache_key)
        
        # ğŸ—‘ï¸ ELIMINAR EXPIRADAS
        for key in expired_keys:
            del self.results_cache[key]
        
        if expired_keys:
            enviar_senal_log(
                level='DEBUG',
                message=f"ğŸ§¹ Cache cleanup | Removed: {len(expired_keys)} | Remaining: {len(self.results_cache)}",
                emisor='acc_flow_controller',
                categoria='acc'
            )
