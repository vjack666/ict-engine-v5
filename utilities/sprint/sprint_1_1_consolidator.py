# ğŸ¯ SPRINT 1.1 CONSOLIDATOR - DEBUG SYSTEM COMPLETE
# Herramienta final para consolidar todas las mejoras del Sprint 1.1

import sys
import os
from pathlib import Path
from datetime import datetime
import json
import subprocess
import argparse
from typing import Dict, List

class Sprint11Consolidator:
    """
    Consolidador final para Sprint 1.1: Debug System & Clean Code

    Funcionalidades:
    - ValidaciÃ³n completa del debug launcher
    - VerificaciÃ³n de migraciÃ³n de prints
    - ConfiguraciÃ³n de console mode
    - Testing de rendering limpio
    - GeneraciÃ³n de reporte de sprint
    """

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.sprint_report = {
            'sprint': '1.1',
            'name': 'Debug System & Clean Code',
            'start_date': datetime.now().isoformat(),
            'status': 'IN_PROGRESS',
            'tasks_completed': [],
            'tasks_pending': [],
            'issues_found': [],
            'metrics': {},
            'next_actions': []
        }

        # ğŸ¯ Tareas del Sprint 1.1
        self.sprint_tasks = {
            'debug_launcher': {
                'name': 'Crear debug_launcher.py con DevTools F12 support',
                'priority': 'CRÃTICA',
                'estimated_hours': 6,
                'deliverable': 'debug_launcher.py'
            },
            'print_migration': {
                'name': 'Migrar 20+ print statements a enviar_senal_log()',
                'priority': 'ALTA',
                'estimated_hours': 4,
                'deliverable': 'Codebase limpio'
            },
            'console_mode': {
                'name': 'Configurar console mode para Textual app',
                'priority': 'MEDIA',
                'estimated_hours': 3,
                'deliverable': 'Console mode configurado'
            },
            'screenshot_tool': {
                'name': 'Implementar screenshot capability',
                'priority': 'BAJA',
                'estimated_hours': 3,
                'deliverable': 'Screenshot tool'
            },
            'rendering_tests': {
                'name': 'Testing intensivo de rendering limpio',
                'priority': 'ALTA',
                'estimated_hours': 3,
                'deliverable': '100+ actualizaciones sin corrupciÃ³n'
            }
        }

    def run_complete_validation(self) -> Dict:
        """Ejecuta validaciÃ³n completa del Sprint 1.1"""
        print("ğŸ” Iniciando validaciÃ³n completa del Sprint 1.1...")

        validation_results = {
            'overall_status': 'UNKNOWN',
            'task_results': {},
            'critical_issues': [],
            'warnings': [],
            'recommendations': []
        }

        # âœ… Validar cada tarea del sprint
        for task_id, task_info in self.sprint_tasks.items():
            print(f"\nğŸ“‹ Validando tarea: {task_info['name']}")

            task_result = self._validate_task(task_id, task_info)
            validation_results['task_results'][task_id] = task_result

            if task_result['status'] == 'COMPLETED':
                self.sprint_report['tasks_completed'].append(task_id)
                print(f"  âœ… COMPLETADA")
            elif task_result['status'] == 'FAILED':
                self.sprint_report['tasks_pending'].append(task_id)
                print(f"  âŒ FALLIDA: {task_result.get('error', 'Unknown error')}")
            else:
                self.sprint_report['tasks_pending'].append(task_id)
                print(f"  â³ PENDIENTE")

        # ğŸ¯ Determinar estado general
        completed_tasks = len(self.sprint_report['tasks_completed'])
        total_tasks = len(self.sprint_tasks)
        completion_rate = (completed_tasks / total_tasks) * 100

        if completion_rate >= 90:
            validation_results['overall_status'] = 'SPRINT_COMPLETE'
        elif completion_rate >= 70:
            validation_results['overall_status'] = 'MOSTLY_COMPLETE'
        elif completion_rate >= 50:
            validation_results['overall_status'] = 'PARTIAL_COMPLETE'
        else:
            validation_results['overall_status'] = 'NEEDS_WORK'

        self.sprint_report['completion_rate'] = completion_rate
        self.sprint_report['status'] = validation_results['overall_status']

        return validation_results

    def _validate_task(self, task_id: str, task_info: Dict) -> Dict:
        """Valida una tarea especÃ­fica del sprint"""
        task_result = {
            'task_id': task_id,
            'status': 'PENDING',
            'checks_passed': 0,
            'checks_total': 0,
            'details': [],
            'error': None
        }

        try:
            if task_id == 'debug_launcher':
                task_result = self._validate_debug_launcher()
            elif task_id == 'print_migration':
                task_result = self._validate_print_migration()
            elif task_id == 'console_mode':
                task_result = self._validate_console_mode()
            elif task_id == 'screenshot_tool':
                task_result = self._validate_screenshot_tool()
            elif task_id == 'rendering_tests':
                task_result = self._validate_rendering_tests()

        except Exception as e:
            task_result['status'] = 'FAILED'
            task_result['error'] = str(e)

        return task_result

    def _validate_debug_launcher(self) -> Dict:
        """Valida que debug_launcher.py estÃ© funcionando"""
        result = {
            'task_id': 'debug_launcher',
            'checks_passed': 0,
            'checks_total': 5,
            'details': [],
            'status': 'PENDING'
        }

        # âœ… Check 1: Archivo existe
        launcher_path = self.project_root / "utilities" / "debug" / "debug_launcher.py"
        if launcher_path.exists():
            result['checks_passed'] += 1
            result['details'].append("âœ… debug_launcher.py existe")
        else:
            result['details'].append("âŒ debug_launcher.py no encontrado")
            result['status'] = 'FAILED'
            return result

        # âœ… Check 2: Imports correctos
        try:
            with open(launcher_path, 'r', encoding='utf-8') as f:
                content = f.read()

            required_imports = [
                'from textual.app import App',
                'dashboard.dashboard_definitivo',
                'class DebugLauncher'
            ]

            imports_ok = all(imp in content for imp in required_imports)
            if imports_ok:
                result['checks_passed'] += 1
                result['details'].append("âœ… Imports correctos")
            else:
                result['details'].append("âŒ Imports faltantes")

        except Exception as e:
            result['details'].append(f"âŒ Error leyendo archivo: {e}")

        # âœ… Check 3: Bindings F12
        if 'F12' in content or 'f12' in content:
            result['checks_passed'] += 1
            result['details'].append("âœ… Binding F12 presente")
        else:
            result['details'].append("âŒ Binding F12 no encontrado")

        # âœ… Check 4: DevTools functionality
        if 'toggle_devtools' in content:
            result['checks_passed'] += 1
            result['details'].append("âœ… DevTools functionality presente")
        else:
            result['details'].append("âŒ DevTools functionality faltante")

        # âœ… Check 5: Launch modes
        launch_modes = ['launch_normal', 'launch_debug', 'launch_console']
        modes_present = sum(1 for mode in launch_modes if mode in content)
        if modes_present >= 2:
            result['checks_passed'] += 1
            result['details'].append(f"âœ… Launch modes ({modes_present}/3)")
        else:
            result['details'].append(f"âŒ Launch modes insuficientes ({modes_present}/3)")

        # ğŸ¯ Determinar estado final
        if result['checks_passed'] >= 4:
            result['status'] = 'COMPLETED'
        elif result['checks_passed'] >= 2:
            result['status'] = 'PARTIAL'
        else:
            result['status'] = 'FAILED'

        return result

    def _validate_print_migration(self) -> Dict:
        """Valida que la migraciÃ³n de prints estÃ© completa"""
        result = {
            'task_id': 'print_migration',
            'checks_passed': 0,
            'checks_total': 4,
            'details': [],
            'status': 'PENDING'
        }

        # âœ… Check 1: Print migration tool existe
        migration_tool_path = self.project_root / "utilities" / "migration" / "print_migration_tool.py"
        if migration_tool_path.exists():
            result['checks_passed'] += 1
            result['details'].append("âœ… Print migration tool existe")
        else:
            result['details'].append("âŒ Print migration tool no encontrado")

        # âœ… Check 2: Escanear prints restantes
        try:
            # Importar herramienta de migraciÃ³n si existe
            if migration_tool_path.exists():
                import importlib.util
                spec = importlib.util.spec_from_file_location("print_migration_tool", migration_tool_path)
                migration_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(migration_module)

                migration_tool = migration_module.PrintMigrationTool(self.project_root)
                scan_results = migration_tool.scan_project()

                total_prints = scan_results['total_prints']
                if total_prints == 0:
                    result['checks_passed'] += 2  # Double points for zero prints
                    result['details'].append("âœ… No hay print statements restantes")
                elif total_prints <= 5:
                    result['checks_passed'] += 1
                    result['details'].append(f"âš ï¸ Quedan {total_prints} prints (aceptable)")
                else:
                    result['details'].append(f"âŒ Quedan {total_prints} prints (demasiados)")
            else:
                result['details'].append("âŒ No se pudo escanear prints")

        except Exception as e:
            result['details'].append(f"âŒ Error escaneando prints: {e}")

        # âœ… Check 3: enviar_senal_log usage
        enviar_senal_log_usage = 0
        try:
            for py_file in self.project_root.rglob("*.py"):
                if py_file.name in ['debug_launcher.py', 'print_migration_tool.py']:
                    continue

                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if 'enviar_senal_log' in content:
                            enviar_senal_log_usage += content.count('enviar_senal_log')
                except:
                    continue

            if enviar_senal_log_usage > 0:
                result['checks_passed'] += 1
                result['details'].append(f"âœ… enviar_senal_log usado {enviar_senal_log_usage} veces")
            else:
                result['details'].append("âŒ enviar_senal_log no encontrado")

        except Exception as e:
            result['details'].append(f"âŒ Error verificando enviar_senal_log: {e}")

        # ğŸ¯ Determinar estado
        if result['checks_passed'] >= 3:
            result['status'] = 'COMPLETED'
        elif result['checks_passed'] >= 2:
            result['status'] = 'PARTIAL'
        else:
            result['status'] = 'FAILED'

        return result

    def _validate_console_mode(self) -> Dict:
        """Valida configuraciÃ³n de console mode"""
        result = {
            'task_id': 'console_mode',
            'checks_passed': 0,
            'checks_total': 3,
            'details': [],
            'status': 'PENDING'
        }

        # âœ… Check 1: Environment variables en debug launcher
        launcher_path = self.project_root / "utilities" / "debug" / "debug_launcher.py"
        if launcher_path.exists():
            try:
                with open(launcher_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                if 'TEXTUAL_CONSOLE' in content:
                    result['checks_passed'] += 1
                    result['details'].append("âœ… TEXTUAL_CONSOLE configurado")
                else:
                    result['details'].append("âŒ TEXTUAL_CONSOLE no configurado")

                if 'TEXTUAL_DEBUG' in content:
                    result['checks_passed'] += 1
                    result['details'].append("âœ… TEXTUAL_DEBUG configurado")
                else:
                    result['details'].append("âŒ TEXTUAL_DEBUG no configurado")

            except Exception as e:
                result['details'].append(f"âŒ Error verificando console mode: {e}")

        # âœ… Check 2: Console mode launch option
        if launcher_path.exists():
            try:
                with open(launcher_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                if 'launch_console' in content:
                    result['checks_passed'] += 1
                    result['details'].append("âœ… Launch console mode disponible")
                else:
                    result['details'].append("âŒ Launch console mode no encontrado")

            except Exception as e:
                result['details'].append(f"âŒ Error verificando launch console: {e}")        # ğŸ¯ Determinar estado
        if result['checks_passed'] >= 2:
            result['status'] = 'COMPLETED'
        elif result['checks_passed'] >= 1:
            result['status'] = 'PARTIAL'
        else:
            result['status'] = 'FAILED'

        return result

    def _validate_screenshot_tool(self) -> Dict:
        """Valida funcionalidad de screenshots"""
        result = {
            'task_id': 'screenshot_tool',
            'checks_passed': 0,
            'checks_total': 2,
            'details': [],
            'status': 'PENDING'
        }

        # âœ… Check 1: Screenshot functionality en debug launcher
        launcher_path = self.project_root / "utilities" / "debug" / "debug_launcher.py"
        if launcher_path.exists():
            try:
                with open(launcher_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                if 'action_screenshot' in content:
                    result['checks_passed'] += 1
                    result['details'].append("âœ… Screenshot action implementado")
                else:
                    result['details'].append("âŒ Screenshot action no encontrado")

                if '_capture_debug_screenshot' in content:
                    result['checks_passed'] += 1
                    result['details'].append("âœ… Screenshot capture function presente")
                else:
                    result['details'].append("âŒ Screenshot capture function faltante")

            except Exception as e:
                result['details'].append(f"âŒ Error verificando screenshot tool: {e}")        # ğŸ¯ Determinar estado
        if result['checks_passed'] >= 1:
            result['status'] = 'COMPLETED'
        else:
            result['status'] = 'FAILED'

        return result

    def _validate_rendering_tests(self) -> Dict:
        """Valida sistema de testing de rendering"""
        result = {
            'task_id': 'rendering_tests',
            'checks_passed': 0,
            'checks_total': 3,
            'details': [],
            'status': 'PENDING'
        }

        # âœ… Check 1: Logging system funcionando
        try:
            from sistema.logging_interface import enviar_senal_log
            result['checks_passed'] += 1
            result['details'].append("âœ… Sistema de logging funcional")
        except ImportError:
            result['details'].append("âŒ Sistema de logging no disponible")

        # âœ… Check 2: Dashboard principal ejecutable
        dashboard_path = self.project_root / "dashboard" / "dashboard_definitivo.py"
        if dashboard_path.exists():
            result['checks_passed'] += 1
            result['details'].append("âœ… Dashboard principal existe")
        else:
            result['details'].append("âŒ Dashboard principal no encontrado")

        # âœ… Check 3: Debug launcher ejecutable
        launcher_path = self.project_root / "utilities" / "debug" / "debug_launcher.py"
        if launcher_path.exists():
            result['checks_passed'] += 1
            result['details'].append("âœ… Debug launcher ejecutable")
        else:
            result['details'].append("âŒ Debug launcher no disponible")

        # ğŸ¯ Determinar estado
        if result['checks_passed'] >= 2:
            result['status'] = 'COMPLETED'
        elif result['checks_passed'] >= 1:
            result['status'] = 'PARTIAL'
        else:
            result['status'] = 'FAILED'

        return result

    def generate_sprint_report(self) -> str:
        """Genera reporte completo del sprint"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.project_root / f"sprint_1_1_report_{timestamp}.json"

        # ğŸ“Š MÃ©tricas adicionales
        self.sprint_report['metrics'] = {
            'completion_rate': self.sprint_report.get('completion_rate', 0),
            'tasks_total': len(self.sprint_tasks),
            'tasks_completed': len(self.sprint_report['tasks_completed']),
            'tasks_pending': len(self.sprint_report['tasks_pending']),
            'validation_timestamp': datetime.now().isoformat()
        }

        # ğŸ¯ PrÃ³ximas acciones recomendadas
        if self.sprint_report['completion_rate'] >= 80:
            self.sprint_report['next_actions'] = [
                "âœ… Sprint 1.1 mayormente completo",
                "ğŸš€ Proceder con Sprint 1.2: Trading Engine Foundation",
                "ğŸ“Š Ejecutar testing final de integraciÃ³n",
                "ğŸ“‹ Actualizar documentation del sistema"
            ]
        else:
            self.sprint_report['next_actions'] = [
                "âš ï¸ Completar tareas pendientes del Sprint 1.1",
                "ğŸ” Resolver issues crÃ­ticos identificados",
                "ğŸ§ª Ejecutar validaciÃ³n nuevamente",
                "ğŸ“‹ Revisar criterios de aceptaciÃ³n"
            ]

        # ğŸ’¾ Guardar reporte
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.sprint_report, f, indent=2, ensure_ascii=False)

        return str(report_path)

    def run_integration_tests(self) -> Dict:
        """Ejecuta tests de integraciÃ³n bÃ¡sicos"""
        print("ğŸ§ª Ejecutando tests de integraciÃ³n...")

        integration_results = {
            'tests_run': 0,
            'tests_passed': 0,
            'tests_failed': 0,
            'test_details': []
        }

        # ğŸ§ª Test 1: Import bÃ¡sico del debug launcher
        try:
            import importlib.util
            launcher_path = self.project_root / "debug_launcher.py"
            if launcher_path.exists():
                spec = importlib.util.spec_from_file_location("debug_launcher", launcher_path)
                launcher_module = importlib.util.module_from_spec(spec)
                # No ejecutamos spec.loader.exec_module() para evitar side effects
                integration_results['tests_run'] += 1
                integration_results['tests_passed'] += 1
                integration_results['test_details'].append("âœ… Debug launcher importable")
            else:
                integration_results['tests_run'] += 1
                integration_results['tests_failed'] += 1
                integration_results['test_details'].append("âŒ Debug launcher no encontrado")
        except Exception as e:
            integration_results['tests_run'] += 1
            integration_results['tests_failed'] += 1
            integration_results['test_details'].append(f"âŒ Error importando debug launcher: {e}")

        # ğŸ§ª Test 2: Sistema de logging funcional
        try:
            from sistema.logging_interface import enviar_senal_log
            enviar_senal_log("INFO", "Test de integraciÃ³n Sprint 1.1", "sprint_consolidator", "test")
            integration_results['tests_run'] += 1
            integration_results['tests_passed'] += 1
            integration_results['test_details'].append("âœ… Sistema de logging funcional")
        except Exception as e:
            integration_results['tests_run'] += 1
            integration_results['tests_failed'] += 1
            integration_results['test_details'].append(f"âŒ Error en sistema de logging: {e}")

        # ğŸ§ª Test 3: Estructura de directorios
        required_dirs = ['dashboard', 'core', 'sistema', 'config']
        dirs_ok = 0
        for dir_name in required_dirs:
            if (self.project_root / dir_name).exists():
                dirs_ok += 1

        integration_results['tests_run'] += 1
        if dirs_ok == len(required_dirs):
            integration_results['tests_passed'] += 1
            integration_results['test_details'].append("âœ… Estructura de directorios correcta")
        else:
            integration_results['tests_failed'] += 1
            integration_results['test_details'].append(f"âŒ Estructura de directorios incompleta ({dirs_ok}/{len(required_dirs)})")

        return integration_results


def main():
    """FunciÃ³n principal del consolidator"""
    parser = argparse.ArgumentParser(description="Sprint 1.1 Consolidator")
    parser.add_argument("--project-root", type=Path, default=Path.cwd(),
                       help="Root directory of the project")
    parser.add_argument("--validation-only", action="store_true",
                       help="Only run validation, skip integration tests")
    parser.add_argument("--integration-only", action="store_true",
                       help="Only run integration tests")
    parser.add_argument("--report", action="store_true",
                       help="Generate detailed sprint report")

    args = parser.parse_args()

    # ğŸ¯ Inicializar consolidator
    consolidator = Sprint11Consolidator(args.project_root)

    print("ğŸ¯ SPRINT 1.1 CONSOLIDATOR - DEBUG SYSTEM & CLEAN CODE")
    print("=" * 60)

    if args.integration_only:
        # ğŸ§ª Solo integration tests
        print("ğŸ§ª Ejecutando solo tests de integraciÃ³n...")
        integration_results = consolidator.run_integration_tests()

        print(f"\nğŸ“Š RESULTADOS DE INTEGRACIÃ“N:")
        print(f"  ğŸ§ª Tests ejecutados: {integration_results['tests_run']}")
        print(f"  âœ… Tests pasados: {integration_results['tests_passed']}")
        print(f"  âŒ Tests fallidos: {integration_results['tests_failed']}")

        for detail in integration_results['test_details']:
            print(f"  {detail}")

    elif args.validation_only:
        # ğŸ” Solo validaciÃ³n
        print("ğŸ” Ejecutando solo validaciÃ³n de tareas...")
        validation_results = consolidator.run_complete_validation()

        print(f"\nğŸ“Š RESULTADOS DE VALIDACIÃ“N:")
        print(f"  ğŸ“ˆ Estado general: {validation_results['overall_status']}")
        print(f"  ğŸ“‹ Completitud: {consolidator.sprint_report['completion_rate']:.1f}%")
        print(f"  âœ… Tareas completadas: {len(consolidator.sprint_report['tasks_completed'])}")
        print(f"  â³ Tareas pendientes: {len(consolidator.sprint_report['tasks_pending'])}")

    else:
        # ğŸš€ ValidaciÃ³n completa + integration tests
        print("ğŸš€ Ejecutando validaciÃ³n completa y tests de integraciÃ³n...")

        # ğŸ” ValidaciÃ³n de tareas
        validation_results = consolidator.run_complete_validation()

        # ğŸ§ª Integration tests
        integration_results = consolidator.run_integration_tests()

        # ğŸ“Š Resultados combinados
        print(f"\nğŸ‰ RESUMEN COMPLETO DEL SPRINT 1.1:")
        print("=" * 50)

        print(f"\nğŸ“‹ VALIDACIÃ“N DE TAREAS:")
        print(f"  ğŸ“ˆ Estado general: {validation_results['overall_status']}")
        print(f"  ğŸ“Š Completitud: {consolidator.sprint_report['completion_rate']:.1f}%")
        print(f"  âœ… Tareas completadas: {len(consolidator.sprint_report['tasks_completed'])}")
        print(f"  â³ Tareas pendientes: {len(consolidator.sprint_report['tasks_pending'])}")

        print(f"\nğŸ§ª TESTS DE INTEGRACIÃ“N:")
        print(f"  ğŸ§ª Tests ejecutados: {integration_results['tests_run']}")
        print(f"  âœ… Tests pasados: {integration_results['tests_passed']}")
        print(f"  âŒ Tests fallidos: {integration_results['tests_failed']}")

        # ğŸ¯ Recomendaciones
        print(f"\nğŸ¯ PRÃ“XIMAS ACCIONES:")
        for action in consolidator.sprint_report['next_actions']:
            print(f"  {action}")

        # ğŸ† Estado final
        if consolidator.sprint_report['completion_rate'] >= 80:
            print(f"\nğŸ† Â¡SPRINT 1.1 EXITOSO!")
            print("ğŸš€ Listo para proceder con Sprint 1.2: Trading Engine Foundation")
        else:
            print(f"\nâš ï¸ SPRINT 1.1 REQUIERE ATENCIÃ“N")
            print("ğŸ”§ Completar tareas pendientes antes de continuar")

    # ğŸ“Š Generar reporte si se solicita
    if args.report:
        report_path = consolidator.generate_sprint_report()
        print(f"\nğŸ“Š Reporte detallado guardado en: {report_path}")


if __name__ == "__main__":
    main()
